
import TensorFlow
import fastai_layers
import fully_connected


/**
 * # Activation explosion and vanishing
 * 
 */

/**
 * To understand why initialization is important in a neural net, 
 * we'll focus on the basic operation you have there: matrix multiplications. 
 * So let's just take a vector x, and a matrix a initialized randomly, 
 * then multiply them 100 times (as if we had 100 layers).
 */

// var x = TF(randomNormal: [512, 1])
// let a = TF(randomNormal: [512,512])
// for i in 0..<100 { x = a • x }
// print("(x.mean(),x.std())")

/**
 * The problem you'll get with that is activation explosion: 
 * very soon, your activations will go to nan. 
 * We can even ask the loop to break when that first happens:
 */
var x = TF(randomNormal: [512, 1])
let a = TF(randomNormal: [512,512])
for i in 0..<100 {
    x = a • x
    if x.std().scalarized().isNaN {
        print(i)
        break
    }
}
print("mean: \(x.mean())","std: \(x.std())")
// 27
// mean: -nan std: -nan
// It only takes around 30 multiplications! before it explode

var x2 = TF(randomNormal: [512, 1])
let a2 = TF(randomNormal: [512,512]) * 0.01
for i in 0..<100 { x2 = a2 • x2 }
print("x2 mean: \(x2.mean())",",x2 std: \(x2.std())")

// On the other hand, 
// if you initialize your activations with a scale that is too low, 
// then you'll get another problem: vanishing gradient
// x2 mean: 0.0 ,x2 std: 0.0

// Here, every activation vanished to 0. So to avoid that problem, people have come with 
// several strategies to initialize their weight matrices, such as:
// - use a standard deviation that will make sure x and Ax have exactly the same scale
// - use an orthogonal matrix to initialize the weight (orthogonal matrices have the 
// special property that they preserve the L2 norm, so x and Ax would have the same sum of squares in that case)
// - use [spectral normalization](https://arxiv.org/pdf/1802.05957.pdf) 
// on the matrix A  (the spectral norm of A is the least possible number M such that `matmul(A,x).norm() <= M*x.norm()`
// so dividing A by this M insures you don't overflow. You can still vanish with this)

// The magic number for scaling¶
// Xavier initialization sqrt(x)
//
// - use a standard deviation that will make sure x and Ax have exactly the same scale

// Here we will focus on the first one, which is the Xavier initialization. 
// It tells us that we should use a scale equal to 1/sqrt(n_in) where n_in is the number of inputs of our matrix.
var x3 = TF(randomNormal: [512, 1])
let a3 = TF(randomNormal: [512,512]) / sqrt(512)
for i in 0..<100 { x3 = a3 • x3 }
print("x3 mean: \(x3.mean())",",x3 std: \(x3.std())")
//And indeed it works. Note that this magic number isn't very far from the 0.01 we had earlier.
print("value of 1/sqrt: \(1 / sqrt(512))")
// x3 mean: -0.38687468 ,x3 std: 4.121207
// value of 1/sqrt: 0.044194173824159216
/**
 * But where does it come from? It's not that mysterious if you remember the definition of 
 * the matrix multiplication. When we do `y = matmul(a, x)`, the coefficients of `y` are defined by

$$y_{i} = a_{i,0} x_{0} + a_{i,1} x_{1} + \cdots + a_{i,n-1} x_{n-1} = \sum_{k=0}^{n-1} a_{i,k} x_{k}$$

or in code:
```
for i in 0..<a.shape[0] {
    for k in 0..<b.shape[1] {
        y[i][0] += a[i][k] * x[k][0]
    }
}
```

Now at the very beginning, our `x` vector has a mean of roughly 0. and a standard deviation of roughly 1. 
(since we picked it that way).
 */
var x4 = TF(randomNormal: [512, 1])
print("x4 mean: \(x4.mean())",",x4 std: \(x4.std())")
/** 
 * NB: This is why it's extremely important to normalize your inputs in Deep Learning, 
 * the initialization rules have been designed with inputs that have a mean 0. and a standard deviation of 1.
If you need a refresher from your statistics course, the mean is the sum of all 
the elements divided by the number of elements (a basic average). 
The standard deviation shows whether the data points stay close to the mean or are far away from it. 
It's computed by the following formula:

$$\sigma = \sqrt{\frac{1}{n}\left[(x_{0}-m)^{2} + (x_{1}-m)^{2} + \cdots + (x_{n-1}-m)^{2}\right]}$$

where m is the mean and $\sigma$ (the greek letter sigma) is the standard deviation. 
To avoid that square root, we also often consider a quantity called the variance, which is $\sigma$ squared. 
Here we have a mean of 0, so the variance is just the mean of x squared, 
and the standard deviation is its square root.
 * 
 */
var mean = Float()
var sqr = Float()
for i in 0..<100 {
    let x = TF(randomNormal: [512, 1])
    let a = TF(randomNormal: [512, 512])
    let y = a • x
    mean += y.mean().scalarized()
    sqr  += pow(y, 2).mean().scalarized()
}
//(mean/100, sqr/100)
print("mean: \(mean/100)","std: \(sqrt(sqr/100))")
//mean: -0.05984683 std: 22.504618
// Now that looks very close to the dimension of our matrix 512. 
// And that's no coincidence! When you compute y, you sum 512 product 
// of one element of a by one element of x. So what's the mean and the standard 
// deviation of such a product of one element of `a` by one element of `x`? 
// We can show mathematically that as long as the elements in `a` and 
// the elements in `x` are independent, the mean is 0 and the std is 1.

// This can also be seen experimentally:
var mean2 = Float()
var sqr2 = Float()
for i in 0..<10000 {
    let x = TF(randomNormal: [])
    let a = TF(randomNormal: [])
    let y = a * x
    mean2 += y.scalarized()
    sqr2  += pow(y, 2).scalarized()
}
//(mean/10000,sqrt(sqr/10000))
print("mean2: \(mean2/10000)","std2: \(sqrt(sqr2/10000))")
// mean2: 0.0006803129 std2: 1.0157206
// Then we sum 512 of those things that have a mean of zero, and a variance of 1, 
// so we get something that has a mean of 0, and variance of 512. 
// To go to the standard deviation, we have to add a square root, hence `sqrt(512)` being our magic number.

// If we scale the weights of the matrix `a` and divide them by this `sqrt(512)`, 
// it will give us a `y` of scale 1, and repeating the product as many 
// times as we want and it won't overflow or vanish.
/**
 *  Add ReLU in the mix
 * We can reproduce the previous experiment with a ReLU, to see that this time, 
 * the mean shifts and the variance becomes 0.5. 
 * This time the magic number will be `math.sqrt(2/512)` to properly scale the weights of the matrix.
 */
var mean3 = Float()
var sqr3 = Float()
for i in 0..<10000 {
    let x = TF(randomNormal: [])
    let a = TF(randomNormal: [])
    var y = (a*x).scalarized()
    y = y < 0 ? 0 : y
    mean3 += y
    sqr3  += pow(y, 2)
}
//(mean: mean/10000, sqrt: sqr/10000)
print("mean3: \(mean3/10000)","sqrt3: \((sqr3/10000))")
// mean3: 0.32244557 sqrt3: 0.52960145
// We can double check by running the experiment on the whole matrix product. 
// The variance becomes 512/2 this time:
var mean4 = Float()
var sqr4 = Float()
for i in 0..<100 {
    let x = TF(randomNormal: [512, 1])
    let a = TF(randomNormal: [512, 512])
    var y = a • x
    y = max(y, TF(zeros: y.shape))
    mean4 += y.mean().scalarized()
    sqr4  += pow(y, 2).mean().scalarized()
}
//(mean: mean/100, sqrt: sqr/100)
print("with ReLU mean4: \(mean4/100)","sqrt4: \((sqr4/100))")
// with ReLU mean4: 9.0338335 sqrt4: 254.71822
// Or that scaling the coefficient with the magic number gives us a scale of 1.
var mean5 = Float()
var sqr5 = Float()
for i in 0..<100 {
    let x = TF(randomNormal: [512, 1])
    let a = TF(randomNormal: [512, 512]) * sqrt(2/512) //magic number
    var y = a • x
    y = max(y, TF(zeros: y.shape))
    mean5 += y.mean().scalarized()
    sqr5  += pow(y, 2).mean().scalarized()
}
//(mean: mean/100, sqrt: sqr/100)
print("with ReLU mean5: \(mean5/100)","sqrt5: \((sqr5/100))")
// with ReLU mean5: 0.5623651 sqrt5: 0.9985564
// The math behind is a tiny bit more complex, and you can find everything in the 
// [Kaiming](https://arxiv.org/abs/1502.01852) and 
// the [Xavier](http://proceedings.mlr.press/v9/glorot10a.html) 
// paper but this gives the intuition behind those results.
